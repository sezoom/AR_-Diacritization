{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-03T09:40:18.302195Z",
     "start_time": "2025-04-03T09:40:18.298598Z"
    }
   },
   "source": "print(\"Hello World\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T10:17:44.478047Z",
     "start_time": "2025-04-03T10:17:41.339674Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install pandas datasets",
   "id": "63a99d09a957a28c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./.venv/lib/python3.11/site-packages (2.2.3)\r\n",
      "Collecting datasets\r\n",
      "  Obtaining dependency information for datasets from https://files.pythonhosted.org/packages/b4/83/50abe521eb75744a01efe2ebe836a4b61f4df37941a776f650f291aabdf9/datasets-3.5.0-py3-none-any.whl.metadata\r\n",
      "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\r\n",
      "Requirement already satisfied: numpy>=1.23.2 in ./.venv/lib/python3.11/site-packages (from pandas) (2.2.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas) (2025.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.11/site-packages (from pandas) (2025.1)\r\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from datasets) (3.17.0)\r\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\r\n",
      "  Obtaining dependency information for pyarrow>=15.0.0 from https://files.pythonhosted.org/packages/a0/55/f1a8d838ec07fe3ca53edbe76f782df7b9aafd4417080eebf0b42aab0c52/pyarrow-19.0.1-cp311-cp311-macosx_12_0_arm64.whl.metadata\r\n",
      "  Using cached pyarrow-19.0.1-cp311-cp311-macosx_12_0_arm64.whl.metadata (3.3 kB)\r\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\r\n",
      "  Obtaining dependency information for dill<0.3.9,>=0.3.0 from https://files.pythonhosted.org/packages/c9/7a/cef76fd8438a42f96db64ddaa85280485a9c395e7df3db8158cfec1eee34/dill-0.3.8-py3-none-any.whl.metadata\r\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\r\n",
      "Requirement already satisfied: requests>=2.32.2 in ./.venv/lib/python3.11/site-packages (from datasets) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./.venv/lib/python3.11/site-packages (from datasets) (4.67.1)\r\n",
      "Collecting xxhash (from datasets)\r\n",
      "  Obtaining dependency information for xxhash from https://files.pythonhosted.org/packages/8c/0c/7c3bc6d87e5235672fcc2fb42fd5ad79fe1033925f71bf549ee068c7d1ca/xxhash-3.5.0-cp311-cp311-macosx_11_0_arm64.whl.metadata\r\n",
      "  Using cached xxhash-3.5.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (12 kB)\r\n",
      "Collecting multiprocess<0.70.17 (from datasets)\r\n",
      "  Obtaining dependency information for multiprocess<0.70.17 from https://files.pythonhosted.org/packages/50/15/b56e50e8debaf439f44befec5b2af11db85f6e0f344c3113ae0be0593a91/multiprocess-0.70.16-py311-none-any.whl.metadata\r\n",
      "  Using cached multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\r\n",
      "Collecting fsspec[http]<=2024.12.0,>=2023.1.0 (from datasets)\r\n",
      "  Obtaining dependency information for fsspec[http]<=2024.12.0,>=2023.1.0 from https://files.pythonhosted.org/packages/de/86/5486b0188d08aa643e127774a99bac51ffa6cf343e3deb0583956dca5b22/fsspec-2024.12.0-py3-none-any.whl.metadata\r\n",
      "  Using cached fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\r\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.11/site-packages (from datasets) (3.11.12)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in ./.venv/lib/python3.11/site-packages (from datasets) (0.28.1)\r\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.11/site-packages (from datasets) (24.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.11/site-packages (from datasets) (6.0.2)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets) (2.4.6)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.2)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets) (25.1.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.5.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets) (6.1.0)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets) (0.2.1)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.18.3)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.11/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\r\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\r\n",
      "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m491.2/491.2 kB\u001B[0m \u001B[31m9.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hUsing cached dill-0.3.8-py3-none-any.whl (116 kB)\r\n",
      "Using cached multiprocess-0.70.16-py311-none-any.whl (143 kB)\r\n",
      "Using cached pyarrow-19.0.1-cp311-cp311-macosx_12_0_arm64.whl (30.7 MB)\r\n",
      "Using cached xxhash-3.5.0-cp311-cp311-macosx_11_0_arm64.whl (30 kB)\r\n",
      "Using cached fsspec-2024.12.0-py3-none-any.whl (183 kB)\r\n",
      "Installing collected packages: xxhash, pyarrow, fsspec, dill, multiprocess, datasets\r\n",
      "  Attempting uninstall: fsspec\r\n",
      "    Found existing installation: fsspec 2025.2.0\r\n",
      "    Uninstalling fsspec-2025.2.0:\r\n",
      "      Successfully uninstalled fsspec-2025.2.0\r\n",
      "Successfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 pyarrow-19.0.1 xxhash-3.5.0\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T09:40:27.275096Z",
     "start_time": "2025-04-03T09:40:26.344090Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "sent = 'With the new joiners, the university’s student population now stands at 259.'\n",
    "word_tokenize(sent)"
   ],
   "id": "858d647b5098b6ac",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['With',\n",
       " 'the',\n",
       " 'new',\n",
       " 'joiners',\n",
       " ',',\n",
       " 'the',\n",
       " 'university',\n",
       " '’',\n",
       " 's',\n",
       " 'student',\n",
       " 'population',\n",
       " 'now',\n",
       " 'stands',\n",
       " 'at',\n",
       " '259',\n",
       " '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T11:08:02.865839Z",
     "start_time": "2025-04-03T11:08:02.862960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def computeDatasetStatistics(df,columnName):\n",
    "\n",
    "    total_sentences = 0\n",
    "    total_words = 0\n",
    "    vocabulary = set()\n",
    "\n",
    "    # Define a regex pattern to split sentences on common sentence-ending punctuation marks.\n",
    "    # This pattern covers English (. ! ?) and Arabic (؟) punctuation.\n",
    "    sentence_pattern = r'[.!?؟]+'\n",
    "    word_pattern = r'\\b[\\w\\u064B-\\u0652]+\\b'\n",
    "    for transcription in df[columnName]:\n",
    "        # Remove leading/trailing whitespace.\n",
    "        transcription = transcription.strip()\n",
    "        # Split the transcription into sentences.\n",
    "        sentences = re.split(sentence_pattern, transcription)\n",
    "        # Filter out any empty sentences resulting from the split.\n",
    "        sentences = [s for s in sentences if s.strip()]\n",
    "\n",
    "        total_sentences += len(sentences)\n",
    "\n",
    "        for sentence in sentences:\n",
    "            # Use regex to find all \"word\" tokens.\n",
    "            # This finds alphanumeric sequences and can capture words in multiple languages.\n",
    "            words = re.findall(word_pattern, sentence)\n",
    "\n",
    "            total_words += len(words)\n",
    "            # Add each word in lower case to the vocabulary set.\n",
    "            vocabulary.update(word.lower() for word in words)\n",
    "\n",
    "    vocab_size = len(vocabulary)\n",
    "    avg_sent_length = total_words / total_sentences if total_sentences > 0 else 0\n",
    "    return total_sentences,total_words,vocab_size, avg_sent_length\n",
    "\n"
   ],
   "id": "e6a7eb3156e880b1",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T11:08:06.254625Z",
     "start_time": "2025-04-03T11:08:06.210464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "inputFile=\"datasets/tunswitch.csv\"\n",
    "# Load the dataset from the CSV file.\n",
    "df = pd.read_csv(inputFile)\n",
    "total_sentences, total_words, vocab_size, avg_sent_length = computeDatasetStatistics(df,columnName='transcription')\n",
    "print(\"# sentences:\", total_sentences)\n",
    "print(\"# words:\", total_words)\n",
    "print(\"Vocabulary size:\", vocab_size)\n",
    "print(\"Avg sent length:\", avg_sent_length)"
   ],
   "id": "5425803bc036f23e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# sentences: 5487\n",
      "# words: 75735\n",
      "Vocabulary size: 14441\n",
      "Avg sent length: 13.802624384909787\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T11:08:51.875733Z",
     "start_time": "2025-04-03T11:08:24.267136Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset,concatenate_datasets\n",
    "\n",
    "dataset = load_dataset(\"MBZUAI/ClArTTS\")\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(dataset)"
   ],
   "id": "b4a260926c96dfd3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'file', 'audio', 'sampling_rate', 'duration'],\n",
      "        num_rows: 9500\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'file', 'audio', 'sampling_rate', 'duration'],\n",
      "        num_rows: 205\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T11:09:28.654981Z",
     "start_time": "2025-04-03T11:09:28.646144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if \"train\" in dataset and \"test\" in dataset:\n",
    "    merged_dataset = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]])\n",
    "    print(\"Merged dataset:\")\n",
    "    print(merged_dataset)\n",
    "else:\n",
    "    print(\"The dataset does not contain both 'train' and 'test' splits.\")"
   ],
   "id": "8c355c508c81985a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataset:\n",
      "Dataset({\n",
      "    features: ['text', 'file', 'audio', 'sampling_rate', 'duration'],\n",
      "    num_rows: 9705\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T11:09:30.956902Z",
     "start_time": "2025-04-03T11:09:30.896349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "total_sentences, total_words, vocab_size, avg_sent_length = computeDatasetStatistics(merged_dataset,columnName='text')\n",
    "print(\"# sentences:\", total_sentences)\n",
    "print(\"# words:\", total_words)\n",
    "print(\"Vocabulary size:\", vocab_size)\n",
    "print(\"Avg sent length:\", avg_sent_length)"
   ],
   "id": "745501158d7b7f95",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# sentences: 19963\n",
      "# words: 77995\n",
      "Vocabulary size: 22028\n",
      "Avg sent length: 3.906977909131894\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T11:11:07.896518Z",
     "start_time": "2025-04-03T11:11:07.833199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "inputFile=\"datasets/arzEn.csv\"\n",
    "# Load the dataset from the CSV file.\n",
    "df = pd.read_csv(inputFile)\n",
    "total_sentences, total_words, vocab_size, avg_sent_length = computeDatasetStatistics(df,columnName='transcription')\n",
    "print(\"# sentences:\", total_sentences)\n",
    "print(\"# words:\", total_words)\n",
    "print(\"Vocabulary size:\", vocab_size)\n",
    "print(\"Avg sent length:\", avg_sent_length)"
   ],
   "id": "302f3039eff56b93",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# sentences: 9415\n",
      "# words: 103785\n",
      "Vocabulary size: 10803\n",
      "Avg sent length: 11.023366967604886\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T11:11:59.024052Z",
     "start_time": "2025-04-03T11:11:31.449732Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from datasets import load_dataset,concatenate_datasets\n",
    "\n",
    "dataset = load_dataset(\"herwoww/mdpc\")\n",
    "# Display basic information about the dataset\n",
    "print(dataset)"
   ],
   "id": "eef1577a53095cfe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['audio', 'transcription'],\n",
      "        num_rows: 60677\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['audio', 'transcription'],\n",
      "        num_rows: 5164\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T11:13:17.753242Z",
     "start_time": "2025-04-03T11:13:17.745786Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if \"train\" in dataset and \"test\" in dataset:\n",
    "    merged_dataset = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]])\n",
    "    print(\"Merged dataset:\")\n",
    "    print(merged_dataset)\n",
    "else:\n",
    "    print(\"The dataset does not contain both 'train' and 'test' splits.\")"
   ],
   "id": "398466da55517893",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataset:\n",
      "Dataset({\n",
      "    features: ['audio', 'transcription'],\n",
      "    num_rows: 65841\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T11:13:20.200546Z",
     "start_time": "2025-04-03T11:13:20.060725Z"
    }
   },
   "cell_type": "code",
   "source": [
    "total_sentences, total_words, vocab_size, avg_sent_length = computeDatasetStatistics(merged_dataset,columnName='transcription')\n",
    "print(\"# sentences:\", total_sentences)\n",
    "print(\"# words:\", total_words)\n",
    "print(\"Vocabulary size:\", vocab_size)\n",
    "print(\"Avg sent length:\", avg_sent_length)"
   ],
   "id": "bd55f275740bbce7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# sentences: 65841\n",
      "# words: 135509\n",
      "Vocabulary size: 3432\n",
      "Avg sent length: 2.0581248765966493\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T11:30:22.396815Z",
     "start_time": "2025-04-03T11:30:19.896212Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install arabic_buckwalter_transliteration",
   "id": "515a430617cb628c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting arabic_buckwalter_transliteration\r\n",
      "  Obtaining dependency information for arabic_buckwalter_transliteration from https://files.pythonhosted.org/packages/69/63/223032c9cbda5dd7d15dc34138d0cd16ea2318572d4ec1219c1f49507d25/arabic_buckwalter_transliteration-1.0.5-py3-none-any.whl.metadata\r\n",
      "  Downloading arabic_buckwalter_transliteration-1.0.5-py3-none-any.whl.metadata (414 bytes)\r\n",
      "Downloading arabic_buckwalter_transliteration-1.0.5-py3-none-any.whl (4.0 kB)\r\n",
      "Installing collected packages: arabic_buckwalter_transliteration\r\n",
      "Successfully installed arabic_buckwalter_transliteration-1.0.5\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T11:42:27.431490Z",
     "start_time": "2025-04-03T11:42:27.405971Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from arabic_buckwalter_transliteration.transliteration import buckwalter_to_arabic, arabic_to_buckwalter\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize a list to store the extracted data\n",
    "data = []\n",
    "\n",
    "# Define a regex pattern to capture two quoted fields per line.\n",
    "# This pattern expects a line starting with a quoted string, whitespace, then another quoted string.\n",
    "pattern = re.compile(r'^\"([^\"]+)\"\\s+\"([^\"]+)\"$')\n",
    "\n",
    "# Replace 'data.txt' with the path to your file.\n",
    "with open(\"datasets/ASCorthographic-transcript.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue  # Skip empty lines\n",
    "        match = pattern.match(line)\n",
    "        if match:\n",
    "            file_name, transcription = match.groups()\n",
    "            data.append({\"file_name\": file_name, \"transcription\": transcription})\n",
    "        else:\n",
    "            print(\"Line did not match the expected format:\", line)\n",
    "\n",
    "# Create a pandas DataFrame from the list of dictionaries\n",
    "df = pd.DataFrame(data)\n",
    "sentence_AR=[]\n",
    "#converting to arabic languange\n",
    "for sentence in df[\"transcription\"]:\n",
    "    sentAr=buckwalter_to_arabic(sentence)\n",
    "    sentence_AR.append(sentAr)\n",
    "\n",
    "df[\"transcription_AR\"]=sentence_AR\n"
   ],
   "id": "eb5ed876f12dae0",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T11:42:30.025399Z",
     "start_time": "2025-04-03T11:42:30.008314Z"
    }
   },
   "cell_type": "code",
   "source": [
    "total_sentences, total_words, vocab_size, avg_sent_length = computeDatasetStatistics(df,columnName='transcription_AR')\n",
    "print(\"# sentences:\", total_sentences)\n",
    "print(\"# words:\", total_words)\n",
    "print(\"Vocabulary size:\", vocab_size)\n",
    "print(\"Avg sent length:\", avg_sent_length)"
   ],
   "id": "9a06c5c00954d5e2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# sentences: 1813\n",
      "# words: 16019\n",
      "Vocabulary size: 10126\n",
      "Avg sent length: 8.835631549917265\n"
     ]
    }
   ],
   "execution_count": 54
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
